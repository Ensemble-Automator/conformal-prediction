{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN confidence prediction using Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "# !pip install pyyaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from dqnetwork import DQNetwork\n",
    "from agent import Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir, getcwd\n",
    "from os.path import isabs, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the environment\n",
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load Expert & Evaluation Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n"
     ]
    }
   ],
   "source": [
    "behavior_agent = Agent(path=\"../../models/behavior_DQN_policy.pth\") #loads behavior policy\n",
    "eval_agent   = Agent(path=\"../../models/evaluation_DQN_policy.pth\") #loads evaluation policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Running Trajectory rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(b_agent:Agent, e_agent:Agent):\n",
    "    \"\"\"\n",
    "    Generate Probability for an evaluation agent w.r.t behavior agent.\n",
    "    @Param:\n",
    "    1. b_agent - (Agent) Behavior policy.\n",
    "    2. e_agent - (Agent) Evaluation policy.\n",
    "    @Return:\n",
    "    - actions - (np.array[np.array]) set of actions taken by behavior and evaluation agent.\n",
    "    - probs   - (np.array[np.array[np.array]]) set of probability (shape - env.action_size) \n",
    "                for each timestep both agents.\n",
    "    - states  - (np.array[np.array]) set of states taken by behavior and evaluation agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    #### \n",
    "    states, e_actions, b_actions, e_probs, b_probs = [], [], [], [], []\n",
    "    ####\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False #terminal condition\n",
    "    \n",
    "    while not done:\n",
    "        b_action, b_prob = b_agent.get_action(state, eps=0) #get action using behavior agent.\n",
    "        e_action, e_prob = e_agent.get_action(state, eps=0) #get action using evaluation agent.\n",
    "        \n",
    "        next_state, reward, done, info = env.step(b_action) #rollout from Expert (behavior) policy\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        #### append ####\n",
    "        states.append(state) #append states to feature matrix\n",
    "        \n",
    "        e_actions.append(e_action) #append evaluation policy actions\n",
    "        b_actions.append(b_action) #append behavior policy actions\n",
    "        \n",
    "        b_probs.append(b_prob) #append stochastic actions of behavior agent\n",
    "        e_probs.append(e_prob) #append stochastic actions of evaluation agent\n",
    "        #### append ####\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    print(\"TOTAL REWARD FROM EXPERT POLICY\", total_reward)\n",
    "    \n",
    "    return np.array(states), np.array(e_actions), np.array(b_actions), np.array(e_probs), np.array(b_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL REWARD FROM EXPERT POLICY 199.08916180202797\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, Y_pred, Y_test, eval_prob, behv_prob = generate_trajectory(behavior_agent, eval_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((851, 8), (851,), (851,), (851, 1, 4), (851, 1, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape, Y_pred.shape, Y_test.shape, eval_prob.shape, behv_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
