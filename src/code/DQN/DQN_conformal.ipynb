{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN confidence prediction using Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "# !pip install pyyaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from dqnetwork import DQNetwork\n",
    "from agent import Agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir, getcwd\n",
    "from os.path import isabs, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the environment\n",
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #Enable cuda if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Double Q Network with Prioritized Experience Replay Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load Expert & Evaluation Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded into local and target networks!\n",
      "Model loaded into local and target networks!\n"
     ]
    }
   ],
   "source": [
    "behavior_agent = Agent(path=\"../../models/behavior_DQN_policy.pth\") #loads behavior policy\n",
    "eval_agent   = Agent(path=\"../../models/evaluation_DQN_policy.pth\") #loads evaluation policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Running Trajectory rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(b_agent:Agent, e_agent:Agent):\n",
    "    \"\"\"\n",
    "    Generate Probability for an evaluation agent w.r.t behavior agent.\n",
    "    @Param:\n",
    "    1. b_agent - (Agent) Behavior policy.\n",
    "    2. e_agent - (Agent) Evaluation policy.\n",
    "    @Return:\n",
    "    - actions - (np.array[np.array]) set of actions taken by behavior and evaluation agent.\n",
    "    - probs   - (np.array[np.array[np.array]]) set of probability (shape - env.action_size) \n",
    "                for each timestep both agents.\n",
    "    - states  - (np.array[np.array]) set of states taken by behavior and evaluation agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    #### \n",
    "    states, e_actions, b_actions, e_probs, b_probs = [], [], [], [], []\n",
    "    ####\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False #terminal condition\n",
    "    \n",
    "    while not done:\n",
    "        b_action, b_prob = b_agent.get_action(state, eps=0) #get action using behavior agent.\n",
    "        e_action, e_prob = e_agent.get_action(state, eps=0) #get action using evaluation agent.\n",
    "        \n",
    "        next_state, reward, done, info = env.step(b_action) #rollout from Expert (behavior) policy\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        #### append ####\n",
    "        states.append(state) #append states to feature matrix\n",
    "        \n",
    "        e_actions.append(e_action) #append evaluation policy actions\n",
    "        b_actions.append(b_action) #append behavior policy actions\n",
    "        \n",
    "        b_probs.append(b_prob) #append stochastic actions of behavior agent\n",
    "        e_probs.append(e_prob) #append stochastic actions of evaluation agent\n",
    "        #### append ####\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    print(\"TOTAL REWARD FROM EXPERT POLICY\", total_reward)\n",
    "    assert(total_reward >= 200) #conditioned on truth\n",
    "    return np.array(states), np.array(e_actions), np.array(b_actions), np.array(e_probs), np.array(b_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL REWARD FROM EXPERT POLICY 241.35759855984472\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, Y_eval_train, Y_behv_train, eval_prob, behv_prob = generate_trajectory(behavior_agent, eval_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((582, 8), (585,), (585,), (582, 1, 4), (582, 1, 4))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape, Y_pred.shape, Y_test.shape, eval_prob.shape, behv_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Steps for conformal prediction:\n",
    "<ol>\n",
    "    <li>Calculate nonconformal score using Nearest Centroid algorithm</li>\n",
    "    <li>Calculate p-values corresponding to the current possible prediction/label</li>\n",
    "    <li>Output j as predicted label of the current example with p-value $p_j$ if and only if $p_j > \\epsilon $ </li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(point, centroid):\n",
    "    \"\"\"Calculate the distance between a test point and a centroid point\"\"\"\n",
    "    #assuming point and centroid are tensors\n",
    "    point = np.array(point)\n",
    "    centroid = np.array(centroid)\n",
    "    return np.linalg.norm(point - centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(data):\n",
    "    \"\"\"Find the centroid for one class of object\"\"\"\n",
    "    features, observations = data.shape\n",
    "    if(features > observations):\n",
    "        raise ValueError(\"too few observations\")\n",
    "    \n",
    "    central = []\n",
    "    for i in range(features):\n",
    "        mean = np.mean(data[i])\n",
    "        central.append(mean)\n",
    "    return central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_classes(data, labels):\n",
    "    \"\"\"Split data into N classes based on corresponding label from evaluation agent\"\"\"\n",
    "    hash_table = {}\n",
    "    for i in range(data.shape[0]):\n",
    "        label = labels[i].item() #get class label\n",
    "        if(label in hash_table):\n",
    "            hash_table[label].append(data[i])\n",
    "        else:\n",
    "            hash_table[label] = [data[i]]\n",
    "    \n",
    "    #convert each value to nd.array\n",
    "    for key in hash_table:\n",
    "        hash_table[key] = np.array(hash_table[key])\n",
    "        \n",
    "    return np.array(list(hash_table.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_classes = split_classes(feature_matrix, Y_eval_train) #each category is labeled as chronological class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_centroids = []\n",
    "\n",
    "for matrix in action_classes:\n",
    "    action_centroids.append( centroid(matrix.T) )\n",
    "    \n",
    "action_centroids = np.array(action_centroids) #to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
