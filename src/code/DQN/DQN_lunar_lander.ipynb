{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates an expert behavior policy to solve LunarLander using a double DQN with Prioritized Experience Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dqnetwork import DQNetwork\n",
    "from agent import Agent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'LunarLander-v2'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(8,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #continuous with 8 observations for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space #discrete with 4 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an agent using random policy Ï€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -226.18303588377657\n",
      "Iteration #: 101\n",
      "Ending state:\n",
      "[-0.6700924   0.09227973 -0.15544848  0.12402631  1.490605    0.72833174\n",
      "  0.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    #env.render() #sudo-human\n",
    "    if(done):\n",
    "        break\n",
    "print(\"Reward: {}\\nIteration #: {}\\nEnding state:\\n{}\".format(total_reward, i, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "#To load an agent\n",
    "# agent.load_model(\"../../model/policies/behavior_policy_x.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []  # List with all scores per episode\n",
    "score_d = deque(maxlen=100) #Last 100 episodes\n",
    "NUM_EPISODES = 5_000\n",
    "ENV_SOLVED = 145 #How many mean iterations to stay 'alive' in order to succeed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_i(num_episode, epsilon_min = 0.01):\n",
    "    \"\"\"Simple Epsilon Decay over total number of episodes. Stochastic in nature when summed over\"\"\"\n",
    "    if(num_episode <= 400): #simulate 400 random episodes\n",
    "        return 1\n",
    "    \n",
    "    epsilon = 1.0/(400 - num_episode)\n",
    "    return max(epsilon, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600: Score: 12.607505041093603; Last 100 mean: -0.559782392221064; Epsilon: 0.0111"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPISODES+1):\n",
    "    env_info = env.reset()   # reset the environment\n",
    "    state = env.reset()      # get the initial state\n",
    "    score = 0                # initialize the score\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        epsilon = get_epsilon_i(epoch)\n",
    "        action = agent.get_action(state, epsilon)              # select an action\n",
    "        next_state, reward, done, _ = env.step(action)         # step into next state\n",
    "        transition = (state, action, reward, next_state, done) # set transition\n",
    "        agent.step(transition)                                 # Train the model\n",
    "        score += reward                                        # update the score\n",
    "        state = next_state                                     # roll over the state to next time step\n",
    "        if done:                                               # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    score_d.append(score)\n",
    "    \n",
    "    if(epoch%50 == 0):#Print stats every 50 episodes\n",
    "        print(f\"\\r{epoch}: Score: {score}; Last 100 mean: {np.mean(score_d)}; Epsilon: {epsilon}\", end=\"\")\n",
    "    if(np.mean(score_d) >= ENV_SOLVED):\n",
    "        print(f\"\\n\\nSolved at episode {epoch} with score: {score} and mean: {np.mean(score_d)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Expert policy over episodes\")\n",
    "plt.plot(scores)\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate rolling average\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rolling(window=100).mean()\n",
    "df.plot()\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Rolling Score over episode number\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3) #delay\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "i = 1\n",
    "while True:\n",
    "    action = agent.get_action(state, eps=epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    i+=1 #increment time-step\n",
    "    env.render() #sudo-human\n",
    "    if(done):\n",
    "        break\n",
    "print(\"Reward: {}\\nIteration #: {}\\nEnding state:\\n{}\".format(total_reward, i, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save model:\n",
    "agent.save_model(\"../../model/evaluation_DQN_policy.pth\") #save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
