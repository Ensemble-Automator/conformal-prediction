{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Joey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Joey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #use for stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../../data/spam/spam_ham_dataset.csv\") #Spam or Ham (Spam email) Classification dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label_num']\n",
    "data = data.drop(['label','Unnamed: 0'], axis=1)\n",
    "vocab = set()\n",
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('../../../data/spam/train.csv', index=False, header=False)\n",
    "x_test.to_csv('../../../data/spam/test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words_tokenize(text):\n",
    "    return [word for word in word_tokenize(text) if not word in stop_words and not word in ['?', ',', '.', '\\n']]\n",
    "\n",
    "#data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = torchtext.data.Field(tokenize=remove_stop_words_tokenize, batch_first=True, include_lengths=True, fix_length=10000)\n",
    "Label = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('text', Text), ('label_num', Label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path='../../../data/spam/',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002\n"
     ]
    }
   ],
   "source": [
    "Text.build_vocab(train_data, valid_data, max_size=5000)\n",
    "Label.build_vocab(train_data)\n",
    "print(len(Text.vocab))\n",
    "vocab_size = len(Text.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "        batch_size = 50,\n",
    "        sort_key = lambda x: len(x.text), sort_within_batch = True,\n",
    "        device = device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 50\n",
    "dropout_keep_prob = 0.5\n",
    "embedding_size = 300\n",
    "max_len = 500\n",
    "max_vocab_size=10000\n",
    "seed = 1\n",
    "classes= 2\n",
    "hidden_dim2 = 128\n",
    "num_layers = 2\n",
    "bi_directional = False \n",
    "num_epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):#Simple LSTM RNN based on https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677\n",
    "    #for purposes of testing conformal predictions on an RNN\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_units, hidden_dim, classes_dim, nlayers, pad_index, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_index)\n",
    "        self.rnn = nn.LSTM(embedding_size, lstm_units ,num_layers=nlayers, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(lstm_units, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, classes_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.nlayers= nlayers\n",
    "        self.lstm_units=lstm_units\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.nlayers, batch_size, self.lstm_units).to(device)),\n",
    "                Variable(torch.zeros(self.nlayers, batch_size, self.lstm_units).to(device)))\n",
    "        return h, c\n",
    "    def forward(self, text, text_lengths):\n",
    "        batch_size = text.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
    "        output, (h_n, c_n) = self.rnn(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        out = output_unpacked[:, -1, :]\n",
    "        rel = self.relu(out)\n",
    "        dense1 = self.fc1(rel)\n",
    "        drop = self.dropout(dense1)\n",
    "        preds = self.fc2(drop)\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = LSTM(vocab_size, embedding_size, 93, hidden_dim2, 2, num_layers, 0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(5002, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 93, num_layers=2)\n",
       "  (fc1): Linear(in_features=93, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = (predictions == target)\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text.to(device), text_lengths.to(device))\n",
    "        loss = criterion(predictions, batch.label_num.squeeze())\n",
    "        acc = accuracy(predictions, batch.label_num)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label_num)\n",
    "            acc = accuracy(predictions, batch.label_num)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights_LTSM_.pth')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.553 | Train Acc: 72.16%\n",
      "\t Val. Loss: 0.585 |  Val. Acc: 72.04%\n",
      "\tTrain Loss: 0.535 | Train Acc: 74.61%\n",
      "\t Val. Loss: 0.576 |  Val. Acc: 72.51%\n",
      "\tTrain Loss: 0.522 | Train Acc: 74.61%\n",
      "\t Val. Loss: 0.571 |  Val. Acc: 72.51%\n",
      "\tTrain Loss: 0.518 | Train Acc: 74.79%\n",
      "\t Val. Loss: 0.568 |  Val. Acc: 72.39%\n",
      "\tTrain Loss: 0.509 | Train Acc: 75.17%\n",
      "\t Val. Loss: 0.564 |  Val. Acc: 72.63%\n"
     ]
    }
   ],
   "source": [
    "run_train(5, rnn, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5051919469199797, 0.7514427853609199)\n"
     ]
    }
   ],
   "source": [
    "rnn.load_state_dict(torch.load('saved_weights_LTSM_.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5891605133102054, 0.7205442232745034)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, test_iterator, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
