{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "V_sgB_5dx1f1"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Actor-Critic Method For explainibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic methods**\n",
    "\n",
    "Actor-Critic methods are [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) methods that represent the policy function independent of the value function. \n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
    "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
    "\n",
    "In this tutorial, both the *Actor* and *Critic* will be represented using one neural network with two outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tT4N3qYviUJr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0512b25e3de7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## Model\n",
    "\n",
    "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. We use model subclassing to define the model. \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
    "\n",
    "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
    "\n",
    "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-947777edee67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Combined actor-critic network.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   def __init__(\n\u001b[1;32m      5\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    # x will be the 128 or num_hidden_units neurons\n",
    "    return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zELFIsgMpzsQ",
    "outputId": "1ad8b2a7-9cd2-48f2-fac4-8f022b669407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Shots  KeyPasses  Offsides  UnsTouches    Passes  performance\n",
      "0  -0.619834  -0.644531 -0.392232   -0.822179 -0.433577     0.098511\n",
      "1   0.021374  -0.644531 -0.392232    0.298974  0.387482     1.098106\n",
      "3  -0.619834  -0.644531 -0.392232   -0.822179 -0.087868    -1.846353\n",
      "5  -0.619834  -0.644531 -0.392232    0.298974 -0.044654    -0.249174\n",
      "7  -0.619834  -0.644531  2.549510    1.420127  0.906046    -0.607725\n",
      "9  -0.619834   0.373149 -0.392232    0.298974  0.473909     0.674364\n",
      "11 -0.619834  -0.644531 -0.392232    0.298974  1.381396    -0.042736\n",
      "13  0.021374  -0.644531 -0.392232    1.420127 -0.952140    -0.901084\n",
      "15  0.662581   0.373149 -0.392232    2.541279  0.257841     0.543982\n",
      "17  0.021374  -0.644531 -0.392232    1.420127 -0.217509     1.358870\n",
      "19  0.662581   1.390830  2.549510    2.541279  0.949259     0.141971\n",
      "21 -0.619834  -0.644531 -0.392232    0.298974 -1.773199    -1.129252\n",
      "23 -0.619834  -0.644531 -0.392232   -0.822179 -1.341063    -0.672916\n",
      "25 -0.619834  -0.644531 -0.392232   -0.822179 -1.470704    -0.911949\n",
      "27 -0.619834  -0.644531 -0.392232   -0.822179 -0.952140    -0.738107\n",
      "28 -0.619834   1.390830 -0.392232   -0.822179  0.862832     0.522252\n",
      "30  0.021374  -0.644531 -0.392232   -0.822179  1.251755     0.044185\n",
      "32 -0.619834  -0.644531 -0.392232   -0.822179  0.733191    -1.346556\n",
      "34  0.021374  -0.644531 -0.392232    0.298974  0.949259    -0.368691\n",
      "36 -0.619834   1.390830 -0.392232   -0.822179  1.856745    -0.357826\n",
      "38 -0.619834   0.373149 -0.392232    0.298974  1.554250     0.728690\n",
      "40  1.944995   0.373149 -0.392232   -0.822179 -0.044654     0.022455\n",
      "42  1.303788   1.390830 -0.392232    0.298974  0.689978     0.772151\n",
      "44  1.303788   1.390830 -0.392232    0.298974 -0.087868     1.500117\n",
      "46  3.868617   3.426190  2.549510    0.298974  0.689978     3.162486\n",
      "48  0.662581   0.373149  2.549510   -0.822179 -1.081781     0.674364\n",
      "50 -0.619834  -0.644531 -0.392232   -0.822179 -0.995354    -0.770702\n",
      "52 -0.619834  -0.644531 -0.392232   -0.822179 -1.038567    -0.814163\n",
      "54 -0.619834  -0.644531 -0.392232   -0.822179 -1.297849    -0.922814\n",
      "56  0.021374  -0.644531 -0.392232   -0.822179 -1.124995     0.337544\n"
     ]
    }
   ],
   "source": [
    "# reading CSV\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df = pd.read_csv(\"soccerdatacsv.csv\")\n",
    "# dropping columns if shots is nan\n",
    "df = df[df['Shots'].notna()]\n",
    "# use shots, tackles, passes, dribbles, disp\n",
    "# adjusting all entries to between 0 and 1 - need to zscore stuff\n",
    "cols = [2,4,7,9,16,27]\n",
    "df = df[df.columns[cols]]\n",
    "df = df.apply(zscore)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoXH1OYXZ-yp",
    "outputId": "4be7a65b-c431-4dff-ce42-5e65ec68d42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_critic_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  192       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  165       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  33        \n",
      "=================================================================\n",
      "Total params: 390\n",
      "Trainable params: 390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set up actor critic network\n",
    "num_actions = 5\n",
    "num_hidden_units = 32\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "model.build((None,5))\n",
    "model.summary()\n",
    "# print(model.layers[0].get_weights())\n",
    "# print(model.layers[1].weights)\n",
    "a = np.array(model.layers[1].get_weights())   \n",
    "model.layers[1].set_weights(a + 0.1)  \n",
    "# start modeling with 5 attributes\n",
    "start_state = np.array([1,1,1,1,1])\n",
    "tensor_state = tf.constant([1,1,1,1,1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcUNhnQBqyTz",
    "outputId": "f1a8161e-0b5a-42e2-9636-d5c0c5449da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.18"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing stuff\n",
    "df.iloc[0]['performance']\n",
    "# print(model.layers[1].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mXTDwPNZ-pJ",
    "outputId": "fca901d1-fa83-47b4-aa79-d79fd6f2c341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor state tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 0. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 1. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 0., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.10710674], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.09047959], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# collecting train data by running simulation\n",
    "# reward function, given action of what parameters on variables to modify calc reward based on how close those vars model the true performance\n",
    "# what is done?? state is like the hidden state \n",
    "# action will be a value between -1 and 1, mulitplied by 10 \n",
    "# time maps to place in state\n",
    "# READ IN CSV = ENVIRONMENT\n",
    "# GLOBAL timestep\n",
    "# use shots, tackles, passes, dribbles, disp\n",
    "step = 0\n",
    "# state = np.array([1,1,1,1,1])\n",
    "# # action is wether to increase or decrease the state exponet\n",
    "# def custom_env_step(action: np.array) -> (np.array, float, bool):\n",
    "#   # check if done\n",
    "#   if step > len(df):\n",
    "#     return np.array([1,1,1]), 2.0, True\n",
    "#   # action affects state\n",
    "#   tempstate = [state[i] + 1 if x == 1 else (state[i] -1 if x == -1 else state[i]) for i,x in enumerate(action) ] \n",
    "#   # calculate the reward, mutliply values by 10 so exponents work bc .5^2 is smaller than .5\n",
    "#   # total = sum(np.power(b,f) for f, b in zip(start_state,save.numpy()[0] ))\n",
    "#   model_predict_perf = sum([np.power(value*10,state[index]) for index,value in enumerate(df.iloc[[step],[2,4,7,9,16]].values[0]) ])\n",
    "#   true_perf = df.iloc[step]['performance']\n",
    "#   reward = 10/ np.absolute(model_predict_perf-true_perf)\n",
    "#   reward = tf.convert_to_tensor(reward)\n",
    "#   reward = tf.cast(reward, tf.float32)\n",
    "#   return tempstate, reward, False\n",
    "def custom_env_step2(action: tf.Tensor) -> (tf.Tensor, float, bool):\n",
    "  # check if done\n",
    "  if step > len(df):\n",
    "    return np.array([1,1,1]), 2.0, True\n",
    "  # action affects state\n",
    "  tempstate = tensor_state + action\n",
    "  print('temp', tempstate)\n",
    "  # calculate the reward, mutliply values by 10 so exponents work bc .5^2 is smaller than .5\n",
    "  fromcsv = tf.convert_to_tensor(df.iloc[[step],[0,1,2,3,4]].values[0])\n",
    "  fromcsv = tf.cast(fromcsv, tf.float32)\n",
    "  model_predict_perf = tf.pow(fromcsv*10,tempstate)\n",
    "  true_perf = tf.convert_to_tensor(df.iloc[step]['performance'])\n",
    "  true_perf = tf.cast(true_perf, tf.float32)\n",
    "  reward = tf.abs(10 / (tf.reduce_sum(model_predict_perf) - true_perf))\n",
    "  return tempstate, reward, False\n",
    "\n",
    "# s,r,b = custom_env_step2(tensor_state)\n",
    "# print('s',s)\n",
    "# print('r',r)\n",
    "losses = []\n",
    "# simulation for train data\n",
    "def run_perfomance_episode(\n",
    "    initial_state: np.array,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> List[tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "  # action_probs = np.array([])\n",
    "  # critic_values = np.array([])\n",
    "  # rewards = np.array([])\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  critic_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "  state = initial_state\n",
    "\n",
    "  for t in range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_values, critic_value = model(state)\n",
    "    b = tf.keras.activations.tanh(action_values)\n",
    "    b = tf.round(b)\n",
    "    print('model actions',b)\n",
    "    # actions_list = [round(e, 0) for e in b.numpy()]\n",
    "\n",
    "    # Store critic values\n",
    "    # squeeze. Removes dimensions of size 1 from the shape of a tensor.\n",
    "    # critic_values = np.append(critic_values,critic_value)\n",
    "    critic_values = critic_values.write(0, tf.squeeze(critic_value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(0, b)\n",
    "    # action_probs =np.append(action_probs,actions_list)\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = custom_env_step2(b)\n",
    "    # state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    # rewards = np.append(rewards,reward)\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "\n",
    "    # if tf.cast(done, tf.bool):\n",
    "    #   break\n",
    "    action_probs = action_probs.stack()\n",
    "    critic_values = critic_values.stack()\n",
    "    rewards = rewards.stack()\n",
    "  return action_probs, critic_values, rewards\n",
    "\n",
    "#store actions, values, rewards in global vars\n",
    "print('tensor state', tensor_state)\n",
    "episoderesults = run_perfomance_episode(tensor_state,model,1 )\n",
    "print(episoderesults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zf341rKYZ-eZ"
   },
   "outputs": [],
   "source": [
    "# calculating actor critic loss, loss function\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "# values from critic, returns computed from rewards\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  # reduce sum is just summing all the elements\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss\n",
    "\n",
    "\n",
    "# getting expected returns or sum of rewards, rewards now are worth more than rewards later, this helps fcn converge\n",
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YVWfkfHDOsmB"
   },
   "outputs": [],
   "source": [
    "# updating NN weights for optimization\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# initial_state: tf.Tensor, \n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: np.array, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(model.trainable_variables)\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards  = run_perfomance_episode(initial_state,model,1 )\n",
    "    print(rewards)\n",
    "    print(get_expected_return(rewards, 0.95))\n",
    "    # return ^^ in tensors to not destory the graph chain\n",
    "    lossey = compute_loss(action_probs, values, rewards)\n",
    "  grads = tape.gradient(lossey, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0qjZDV_Owt7",
    "outputId": "99783df6-d9e4-4bd7-b3b3-0e55a6e38c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "OG new tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 0. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 1. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 0., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.10710674], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.09047959], dtype=float32)>)\n",
      "tf.Tensor([[1. 0. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "new state tf.Tensor([2. 1. 2. 2. 1.], shape=(5,), dtype=float32)\n",
      "episode 1\n",
      "OG new tf.Tensor([2. 1. 2. 2. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 2. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 1., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.55251276], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.06308771], dtype=float32)>)\n",
      "tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "new state tf.Tensor([3. 2. 3. 3. 1.], shape=(5,), dtype=float32)\n",
      "episode 2\n",
      "OG new tf.Tensor([3. 2. 3. 3. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 2. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 1., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8737061], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.06308771], dtype=float32)>)\n",
      "tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "new state tf.Tensor([4. 3. 4. 4. 1.], shape=(5,), dtype=float32)\n",
      "episode 3\n",
      "OG new tf.Tensor([4. 3. 4. 4. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 2. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 1., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.2058563], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.06308771], dtype=float32)>)\n",
      "tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "new state tf.Tensor([5. 4. 5. 5. 1.], shape=(5,), dtype=float32)\n",
      "episode 4\n",
      "OG new tf.Tensor([5. 4. 5. 5. 1.], shape=(5,), dtype=float32)\n",
      "model actions tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "temp tf.Tensor([[2. 2. 2. 2. 1.]], shape=(1, 5), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=array([[[1., 1., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.558436], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.06308771], dtype=float32)>)\n",
      "tf.Tensor([[1. 1. 1. 1. 0.]], shape=(1, 5), dtype=float32)\n",
      "new state tf.Tensor([6. 5. 6. 6. 1.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# putting it all together\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# with tqdm.trange(max_episodes) as t:\n",
    "#manual loop for now\n",
    "new_state = tf.constant([1,1,1,1,1], dtype=tf.float32)\n",
    "# print('OG new', new_state)\n",
    "# episoderesults = run_perfomance_episode(new_state,model,1 )\n",
    "# print(episoderesults)\n",
    "# train_step(new_state,model,optimizer,0.8, max_steps_per_episode  )\n",
    "# print(episoderesults[0][0])\n",
    "# new_state = (episoderesults[0][0] + new_state)[0]\n",
    "# print('new state' , new_state)\n",
    "for i in range(5):\n",
    "  print('episode', i)\n",
    "  print('OG new', new_state)\n",
    "  episoderesults = run_perfomance_episode(new_state,model,1 )\n",
    "  print(episoderesults)\n",
    "  # train_step(new_state,model,optimizer,0.8, max_steps_per_episode  )\n",
    "  print(episoderesults[0][0])\n",
    "  new_state = (episoderesults[0][0] + new_state)[0]\n",
    "  print('new state' , new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8T2x0YCOymR",
    "outputId": "a9269f4e-efc9-463f-d44d-3152dac74c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6198336  -0.64453085 -0.39223227 -0.8221786  -0.43357672]\n",
      "0.09851079871438467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "print(df.iloc[[step],[0,1,2,3,4]].values[0])\n",
    "value = df.iloc[[step],[0,1,2,3,4]].values[0]\n",
    "print(df.iloc[0]['performance'])\n",
    "state = np.array([6., 5.,  6.,  6.0,  1.0])\n",
    "sum([np.power(value*10,state[index]) if value > 0 else 1 for index,value in enumerate(value) ])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lnq9Hzo1Po6X"
   ],
   "name": "Copy of actor_critic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
